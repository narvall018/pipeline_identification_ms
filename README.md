# üî¨ Pipeline d'Identification MS


Pipeline d'identification de compos√©s int√©grant MS1, mobilit√© ionique (CCS) et MS2, avec filtration par r√©plicats, soustraction des blancs, et alignement des √©chantillons.

La piepline travaille sur trois dimensions analytiques :
- La masse exacte (m/z)
- Le temps de r√©tention (RT)  
- Le temps de d√©rive (DT)

### 1.2. Architecture

La pipeline est structur√©e en modules interconnect√©s, chacun responsable d'une √©tape sp√©cifique du traitement des donn√©es.

**Flux de Donn√©es**

Les donn√©es traversent la pipeline selon la s√©quence suivante :
1. D√©tection des pics MS1 dans les donn√©es brutes
2. Traitement des r√©plicats pour valider les pics d√©tect√©s
3. Soustraction des blancs pour √©liminer les contaminations
4. Calcul des valeurs CCS via la calibration
5. Alignement des features entre √©chantillons
6. Identification des compos√©s et validation MS2

**Composants Principaux**

La pipeline s'appuie sur sept modules :

- **PeakDetector** : d√©tection des pics dans les donn√©es brutes
- **ReplicateProcessor** : gestion et validation des r√©plicats
- **BlankProcessor** : traitement et soustraction des blancs
- **CCSCalibrator** : calibration et calcul des CCS
- **FeatureProcessor** : alignement des features entre √©chantillons
- **CompoundIdentifier** : identification des compos√©s
- **MS2Extractor** : extraction et validation des spectres MS2

Chaque composant peut √™tre utilis√© ind√©pendamment ou dans le flux complet de la pipeline.


## 2. Installation ‚öôÔ∏è

### 2.1. Via Conda (Recommand√©)
```bash
# Cloner le repository
git clone https://github.com/votre_username/pipeline_identification_ms.git
cd pipeline_identification_ms

# Cr√©er et activer l'environnement
conda env create -f environment.yml
conda activate ms_pipeline

# V√©rifier l'installation
python -c "import deimos; print(deimos.__version__)"
```

En cas d'erreur avec DEIMoS :
```bash
conda activate ms_pipeline
pip uninstall deimos
pip install git+https://github.com/pnnl/deimos.git
```

### 2.2. Via Pip
```bash
# Cr√©er un environnement virtuel
python -m venv ms_env

# Activer l'environnement
# Sur Windows :
ms_env\Scripts\activate
# Sur Linux/macOS :
source ms_env/bin/activate

# Installer les d√©pendances
pip install -r requirements.txt
```

### 2.3. Pr√©requis

**Fichiers Requis**
- üì• Base de donn√©es NORMAN ([T√©l√©charger ici](https://drive.google.com/file/d/1mZa1r9RZ4Ioy1cILJqIteAz3vUs_UIaU/view))
- üóÇÔ∏è Fichiers blancs dans `data/input/blanks/`

**Structure √† Cr√©er**
```
data/
‚îú‚îÄ‚îÄ input/
‚îÇ   ‚îú‚îÄ‚îÄ samples/          # Vos fichiers .parquet
‚îÇ   ‚îú‚îÄ‚îÄ blanks/          # Vos fichiers blancs
‚îÇ   ‚îú‚îÄ‚îÄ calibration/     # Fichiers de calibration
‚îÇ   ‚îî‚îÄ‚îÄ databases/       # Base NORMAN
```

**Configuration Syst√®me**
- Python 3.8 ou sup√©rieur
- 8 Go RAM minimum recommand√©
- Espace disque : 1 Go minimum pour l'installation# Pipeline d'Identification MS

## 2. Installation ‚öôÔ∏è

### 2.1. Via Conda (Recommand√©)
```bash
# Cloner le repository
git clone https://github.com/votre_username/pipeline_identification_ms.git
cd pipeline_identification_ms

# Cr√©er et activer l'environnement
conda env create -f environment.yml
conda activate ms_pipeline

# V√©rifier l'installation
python -c "import deimos; print(deimos.__version__)"
```

En cas d'erreur avec DEIMoS :
```bash
conda activate ms_pipeline
pip uninstall deimos
pip install git+https://github.com/pnnl/deimos.git
```

### 2.2. Via Pip
```bash
# Cr√©er un environnement virtuel
python -m venv ms_env

# Activer l'environnement
# Sur Windows :
ms_env\Scripts\activate
# Sur Linux/macOS :
source ms_env/bin/activate

# Installer les d√©pendances
pip install -r requirements.txt
```

### 2.3. Pr√©requis

**Fichiers Requis**
- üì• Base de donn√©es NORMAN ([T√©l√©charger ici](https://drive.google.com/file/d/1mZa1r9RZ4Ioy1cILJqIteAz3vUs_UIaU/view))
- üóÇÔ∏è Fichiers blancs dans `data/input/blanks/`

**Structure √† Cr√©er**
```
data/
‚îú‚îÄ‚îÄ input/
‚îÇ   ‚îú‚îÄ‚îÄ samples/          # Vos fichiers .parquet
‚îÇ   ‚îú‚îÄ‚îÄ blanks/          # Vos fichiers blancs
‚îÇ   ‚îú‚îÄ‚îÄ calibration/     # Fichiers de calibration
‚îÇ   ‚îî‚îÄ‚îÄ databases/       # Base NORMAN
```

**Configuration Syst√®me**
- Python 3.8 ou sup√©rieur
- 8 Go RAM minimum recommand√©


## 3. Structure et Configuration üìÅ

### 3.1. Organisation du Projet

```
pipeline_identification_ms/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/  
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ samples/        # Fichiers d'√©chantillons (.parquet)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ blanks/         # Fichiers de blancs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calibration/    # Fichiers de calibration CCS
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ databases/      # Base de donn√©es NORMAN
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ intermediate/       # R√©sultats interm√©diaires
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ samples/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ nom_echantillon/
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ ms1/
‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ peaks_before_blank.parquet
‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ common_peaks.parquet
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ ms2/
‚îÇ   ‚îÇ               ‚îî‚îÄ‚îÄ spectra.parquet
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ output/            # R√©sultats finaux
‚îÇ       ‚îú‚îÄ‚îÄ feature_matrix.parquet
‚îÇ       ‚îú‚îÄ‚îÄ feature_matrix.csv
‚îÇ       ‚îî‚îÄ‚îÄ features_complete.parquet
‚îÇ
‚îú‚îÄ‚îÄ scripts/               # Code source
‚îÇ   ‚îú‚îÄ‚îÄ processing/        # Modules de traitement
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ peak_detection.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ blank_processing.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ replicate_processing.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ccs_calibration.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_matrix.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ identification.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ms2_extraction.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/            # Fonctions utilitaires
‚îÇ       ‚îú‚îÄ‚îÄ io_handlers.py
‚îÇ       ‚îú‚îÄ‚îÄ matching_utils.py
‚îÇ       ‚îî‚îÄ‚îÄ replicate_handling.py
‚îÇ
‚îî‚îÄ‚îÄ logs/                 # Fichiers de logs
```

**Description des Dossiers**

- `data/input/` : Contient toutes les donn√©es d'entr√©e n√©cessaires
  - `samples/` : Vos fichiers d'√©chantillons au format .parquet
  - `blanks/` : Fichiers de blancs analytiques
  - `calibration/` : Donn√©es pour la calibration CCS
  - `databases/` : Base de donn√©es de r√©f√©rence

- `data/intermediate/` : Stocke les r√©sultats de chaque √©tape
  - Organisation par √©chantillon
  - S√©paration MS1/MS2
  - R√©sultats avant/apr√®s soustraction des blancs

- `data/output/` : Contient les r√©sultats finaux
  - Matrices de features
  - Identifications
  - Fichiers aux formats .parquet et .csv

- `scripts/` : Code source de la pipeline
  - `processing/` : Modules principaux de traitement
  - `utils/` : Fonctions utilitaires et helpers


- Fichiers d'√©chantillons : `nom_echantillon.parquet`
- R√©plicats : `nom_echantillon_replicate_1_X.parquet`
- Blancs : `blank_replicate_1.parquet`
- R√©sultats : `nom_explicite.parquet/csv`


### 3.2. Configuration

La configuration de la pipeline est g√©r√©e par des classes d√©di√©es dans `config.py`. Chaque aspect du traitement a sa propre configuration avec des param√®tres par d√©faut optimis√©s.

**Param√®tres Globaux**
- Organisation en classes de configuration
- Gestion des chemins automatis√©e
- Configuration du traitement parall√®le

**Tol√©rances d'Identification**
- Param√®tres MS1 et MS2
- Tol√©rances pour l'alignement
- Seuils de validation

**Optimisation des Performances**
- Nombre de workers parall√®les
- Taille des lots de traitement
- Seuils d'intensit√©

```python
class Config:
    """Configuration du pipeline"""
    
    # D√©tection des pics
    PEAK_DETECTION = PeakDetectionConfig(
        threshold = 100,
        smooth_iterations = 7,
        smooth_radius = {
            'mz': 0,
            'drift_time': 1,
            'retention_time': 0
        },
        peak_radius = {
            'mz': 2,
            'drift_time': 10,
            'retention_time': 0
        }
    )

    # Identification des compos√©s
    IDENTIFICATION = IdentificationConfig(
        database_file = "norman_all_ccs_all_rt_pos_neg_with_ms2.h5",
        database_key = "positive",
        tolerances = {
            'mz_ppm': 5,
            'ccs_percent': 8,
            'rt_min': 0.1
        }
    )

    # Soustraction des blancs
    BLANK_SUBTRACTION = BlankSubtractionConfig(
        mz_ppm = 10,
        dt_tolerance = 0.22,
        rt_tolerance = 0.1,
        dbscan_eps = 1.5,
        dbscan_min_samples = 2
    )

    # Traitement des r√©plicats
    REPLICATE = ReplicateConfig(
        min_replicates = 2,
        mz_ppm = 10,
        dt_tolerance = 0.22,
        rt_tolerance = 0.1
    )

    # Extraction MS2
    MS2_EXTRACTION = MS2ExtractionConfig(
        rt_tolerance = 0.00422,
        dt_tolerance = 0.22,
        max_peaks = 10,
        intensity_scale = 999
    )
```

**Personnalisation des Param√®tres**
- Chaque module a ses param√®tres par d√©faut
- Les valeurs peuvent √™tre ajust√©es selon vos besoins

**Modules Configurables**
- PeakDetection : d√©tection des pics MS1
- Identification : param√®tres d'identification
- BlankSubtraction : soustraction des blancs
- Replicate : gestion des r√©plicats
- MS2Extraction : extraction des spectres MS2
- Alignement : Alignement des √©chantillons



## 4. Composants Principaux üîß

### 4.1. D√©tection des Pics (PeakDetector)

Le PeakDetector est responsable de la d√©tection des pics MS1 dans les donn√©es brutes. Il int√®gre le lissage des donn√©es, la d√©tection des pics et le clustering intra-√©chantillon.

**Utilisation Basique**
```python
from scripts.processing.peak_detection import PeakDetector

# Initialisation
detector = PeakDetector()

# Traitement simple
peaks = detector.process_sample(data)

# Traitement avec param√®tres personnalis√©s
peaks = detector.detect_peaks(
    data,
    threshold=200,              # Seuil plus √©lev√©
    smooth_iterations=7         # Plus d'it√©rations de lissage
)
```

**Param√®tres Cl√©s**
```python
# Configuration par d√©faut
PEAK_DETECTION = {
    'threshold': 50,           # Seuil d'intensit√© minimum
    'smooth_iterations': 7,     # Nombre d'it√©rations pour le lissage
    'smooth_radius': {          # Rayons de lissage
        'mz': 0,
        'drift_time': 1,
        'retention_time': 0
    },
    'peak_radius': {           # Rayons pour la d√©tection
        'mz': 2,
        'drift_time': 10,
        'retention_time': 0
    }
}
```

**Workflow de Traitement**
1. Pr√©paration des donn√©es
   ```python
   prepared_data = detector.prepare_data(data)
   ```

2. D√©tection des pics
   ```python
   peaks = detector.detect_peaks(prepared_data)
   ```

3. Clustering des pics
   ```python
   clustered_peaks = detector.cluster_peaks(peaks)
   ```

**Optimisations Possibles**
- Ajustement du seuil selon le bruit de fond
- Modification des rayons selon la r√©solution de l'instrument
- Param√©trage du clustering selon la complexit√© de l'√©chantillon

**Format des Donn√©es de Sortie**
```python
# Exemple de DataFrame retourn√©
peaks_df = {
    'mz': [123.4567, ...],           # Masse exacte
    'drift_time': [12.3, ...],       # Temps de d√©rive
    'retention_time': [5.6, ...],    # Temps de r√©tention
    'intensity': [1000, ...]         # Intensit√© du pic
}
```

### 4.2. Traitement des R√©plicats (ReplicateProcessor)

Le ReplicateProcessor valide la pr√©sence des pics entre diff√©rents r√©plicats d'un m√™me √©chantillon pour assurer la fiabilit√© des r√©sultats.

**Utilisation Basique**
```python
from scripts.processing.replicate_processing import ReplicateProcessor
from pathlib import Path

# Initialisation
processor = ReplicateProcessor()

# Liste des fichiers r√©plicats
replicate_files = [
    Path("data/input/samples/sample_replicate_1.parquet"),
    Path("data/input/samples/sample_replicate_1_2.parquet"),
    Path("data/input/samples/sample_replicate_1_3.parquet")
]

# Traitement des r√©plicats
results = processor.process_sample_with_replicates(
    sample_name="mon_echantillon",
    replicate_files=replicate_files,
    output_dir=Path("data/output")
)
```

**Configuration**
```python
REPLICATE = {
    'min_replicates': 2,       # Nombre minimum de r√©plicats requis
    'mz_ppm': 10,             # Tol√©rance m/z en ppm
    'dt_tolerance': 0.22,      # Tol√©rance temps de d√©rive
    'rt_tolerance': 0.1,       # Tol√©rance temps de r√©tention
    'dbscan_eps': 0.6,        # Epsilon pour DBSCAN
    'dbscan_min_samples': 2    # √âchantillons minimum pour DBSCAN
}
```

**Workflow de Traitement**
1. Traitement individuel des r√©plicats
```python
peaks_dict, initial_counts = processor.process_replicates(replicate_files)
```

2. Clustering entre r√©plicats
```python
common_peaks = processor.cluster_replicates(peaks_dict)
```

**Format de Sortie**
```python
# Structure des r√©sultats
results = {
    'mz': [],                 # Masses moyennes
    'drift_time': [],         # Temps de d√©rive moyens
    'retention_time': [],     # Temps de r√©tention moyens
    'intensity': [],          # Intensit√©s maximales
    'n_replicates': []        # Nombre de r√©plicats o√π le pic est trouv√©
}
```

**Rapport de Traitement**
- Nombre de pics par r√©plicat
- Pics communs d√©tect√©s
- Statistiques de regroupement



### 4.3. Soustraction des Blancs (BlankProcessor)

Le BlankProcessor √©limine les contaminations et le bruit de fond en soustrayant les pics d√©tect√©s dans les blancs analytiques.

**Utilisation Basique**
```python
from scripts.processing.blank_processing import BlankProcessor
from pathlib import Path

# Initialisation
blank_processor = BlankProcessor()

# Traitement d'un fichier blank individuel
blank_peaks = blank_processor.process_blank_file(blank_file)

# Soustraction des blancs des √©chantillons
clean_peaks = blank_processor.subtract_blank_peaks(sample_peaks, blank_peaks)
```

**M√©thodologie**
1. Traitement des Blancs
   - D√©tection des pics dans les blancs
   - Regroupement des r√©plicats de blancs
   - Validation des pics communs

2. Soustraction
   - Comparaison des pics √©chantillon/blanc
   - Application des tol√©rances
   - Filtrage des pics contaminants

**Configuration**
```python
BLANK_SUBTRACTION = {
    'mz_ppm': 10,              # Tol√©rance masse
    'dt_tolerance': 0.22,      # Tol√©rance temps de d√©rive
    'rt_tolerance': 0.1,       # Tol√©rance temps de r√©tention
    'dbscan_eps': 1.5,         # Epsilon clustering
    'dbscan_min_samples': 2,   # Minimum √©chantillons
}
```

**Param√®tres Importants**
- `mz_ppm`: Tol√©rance pour la comparaison des masses
- `dt_tolerance`: Fen√™tre de temps de d√©rive
- `rt_tolerance`: Fen√™tre de temps de r√©tention
- `cluster_ratio`: Proportion minimum dans les blancs

**Validation des R√©sultats**
- V√©rification du nombre de pics supprim√©s
- Analyse des intensit√©s relatives
- Contr√¥le des pics conserv√©s
```python
# Exemple de validation
print(f"Pics initiaux : {len(sample_peaks)}")
print(f"Pics apr√®s soustraction : {len(clean_peaks)}")
print(f"Pourcentage de suppression : {((len(sample_peaks) - len(clean_peaks)) / len(sample_peaks)) * 100:.1f}%")
```

**Format des Donn√©es de Sortie**
```python
clean_peaks = {
    'mz': [],                  # Masses valid√©es
    'drift_time': [],          # Temps de d√©rive
    'retention_time': [],      # Temps de r√©tention
    'intensity': []            # Intensit√©s
}
```

### 4.4. Features et Alignement (FeatureProcessor)

Le FeatureProcessor aligne les pics entre diff√©rents √©chantillons pour cr√©er une matrice de features commune. Il permet d'identifier et de quantifier les compos√©s √† travers tous les √©chantillons.

**Utilisation Basique**
```python
from scripts.processing.feature_matrix import FeatureProcessor
from pathlib import Path

# Initialisation
processor = FeatureProcessor()

# Alignement et cr√©ation de matrice
matrix, feature_info, raw_files = processor.align_features_across_samples(
    samples_dir=Path("data/intermediate/samples")
)

# Traitement des features avec identification
identifications = processor.process_features(
    feature_df=feature_info,
    raw_files=raw_files,
    identifier=identifier  # Instance de CompoundIdentifier
)
```

**Configuration**
```python
FEATURE_ALIGNMENT = {
    'mz_ppm': 10,              # Tol√©rance masse
    'dt_tolerance': 1.02,      # Tol√©rance temps de d√©rive
    'rt_tolerance': 0.2,       # Tol√©rance temps de r√©tention
    'dbscan_eps': 1.0,         # Epsilon pour clustering
    'dbscan_min_samples': 1    # Minimum d'√©chantillons
}
```

**Cr√©ation de Matrices**
1. Matrice d'Intensit√©s
```python
# Format de la matrice
matrix_df = pd.DataFrame(
    index=sample_names,        # √âchantillons en lignes
    columns=feature_names      # Features en colonnes
)
```

2. Matrice de Features
```python
# Informations sur les features
feature_df = {
    'mz': [],                  # Masse moyenne
    'retention_time': [],      # RT moyen
    'drift_time': [],          # DT moyen
    'intensity': [],           # Intensit√© maximale
    'n_samples': [],           # Nombre d'√©chantillons
    'feature_id': []           # Identifiant unique
}
```

**Optimisation M√©moire**
- Traitement par lots des √©chantillons
- Nettoyage des donn√©es temporaires
- Gestion efficace des grands ensembles
```python
# Exemple d'optimisation
matrix = processor.create_feature_matrix(
    input_dir=input_dir,
    output_dir=output_dir,
    batch_size=100             # Traitement par lots
)
```

**Sorties G√©n√©r√©es**
```
data/output/
‚îú‚îÄ‚îÄ feature_matrix.parquet     # Matrice d'intensit√©s
‚îú‚îÄ‚îÄ feature_matrix.csv        
‚îú‚îÄ‚îÄ features_complete.parquet  # Informations d√©taill√©es
‚îî‚îÄ‚îÄ features_complete.csv
```

**Validation des R√©sultats**
```python
# Statistiques de base
print(f"Nombre de features : {matrix.shape[1]}")
print(f"Nombre d'√©chantillons : {matrix.shape[0]}")
print(f"Taux de remplissage : {(matrix > 0).mean().mean() * 100:.1f}%")
```

### 4.5. Calibration CCS (CCSCalibrator)

Le CCSCalibrator permet de calculer les valeurs CCS (Section Efficace de Collision) √† partir des temps de d√©rive mesur√©s.

**Utilisation Basique**
```python
from scripts.processing.ccs_calibration import CCSCalibrator

# Initialisation avec fichier de calibration
calibrator = CCSCalibrator("path/to/calibration.csv")

# Calibration
calibrator.calibrate()

# Calcul des CCS pour les pics
ccs_values = calibrator.calculate_ccs(peaks_df)
```

**Processus de Calibration**
1. Chargement des donn√©es
```python
# Format du fichier de calibration
calibration_data = {
    'Reference m/z': [],    # m/z de r√©f√©rence
    'Measured m/z': [],     # m/z mesur√©
    'Measured Time': [],    # Temps de d√©rive mesur√©
    'Reference rCCS': [],   # CCS de r√©f√©rence
    'z': []                # √âtat de charge
}
```

2. Validation
- V√©rification de la corr√©lation
- Analyse des r√©sidus
- Contr√¥le de la gamme de calibration

**Standards Recommand√©s**
- Agilent Tune Mix
- Waters Major Mix
- Compos√©s de r√©f√©rence avec CCS connues

### 4.6. Identification (CompoundIdentifier)

Le CompoundIdentifier compare les pics d√©tect√©s avec une base de donn√©es de r√©f√©rence pour identifier les compos√©s.

**Utilisation Basique**
```python
from scripts.processing.identification import CompoundIdentifier

# Initialisation
identifier = CompoundIdentifier()

# Identification des compos√©s
matches = identifier.identify_compounds(peaks_df, output_dir)
```

**Configuration Base de Donn√©es**
```python
IDENTIFICATION = {
    'database_file': "norman_all_ccs_all_rt_pos_neg_with_ms2.h5",
    'database_key': "positive",
    'tolerances': {
        'mz_ppm': 5,           # Tol√©rance masse
        'ccs_percent': 8,      # Tol√©rance CCS
        'rt_min': 0.1          # Tol√©rance RT
    },
    'ms2_score_threshold': 0.5 # Seuil score MS2
}
```

**Format des R√©sultats**
```python
matches = {
    'match_name': [],         # Nom du compos√©
    'formula': [],           # Formule mol√©culaire
    'confidence_level': [],   # Niveau de confiance
    'global_score': [],      # Score global
    'ms2_score': []          # Score MS2 si disponible
}
```

### 4.7. MS2 (MS2Extractor)

Le MS2Extractor extrait et traite les spectres MS2 pour valider les identifications via la comparaison avec des spectres de r√©f√©rence.

**Utilisation Basique**
```python
from scripts.processing.ms2_extraction import MS2Extractor

# Initialisation
extractor = MS2Extractor()

# Extraction d'un spectre MS2
spectra = extractor.extract_ms2_spectrum(
    ms2_data=ms2_data,
    rt=retention_time,
    dt=drift_time
)

# Extraction pour plusieurs matches
matches_with_ms2 = extractor.extract_ms2_for_matches(
    matches_df=matches,
    raw_parquet_path="path/to/raw.parquet",
    output_dir="path/to/output"
)
```

**Configuration**
```python
MS2_EXTRACTION = {
    'rt_tolerance': 0.00422,    # Fen√™tre RT (minutes)
    'dt_tolerance': 0.22,       # Fen√™tre DT (ms)
    'mz_round_decimals': 3,     # Pr√©cision m/z
    'max_peaks': 10,            # Pics maximum
    'intensity_scale': 999      # √âchelle d'intensit√©
}
```

**Processus d'Extraction**
1. S√©lection de la fen√™tre RT/DT
```python
# D√©finition des fen√™tres
rt_window = (rt - rt_tolerance, rt + rt_tolerance)
dt_window = (dt - dt_tolerance, dt + dt_tolerance)
```

2. Traitement du Spectre
```python
# Format du spectre
spectrum = {
    'mz_rounded': [],          # m/z arrondis
    'intensity_normalized': [] # Intensit√©s normalis√©es
}
```

**Comparaison des Spectres**
- Alignement des pics
- Normalisation des intensit√©s
- Calcul des scores de similarit√©

**Format des R√©sultats**
```python
# Structure des donn√©es de sortie
ms2_results = {
    'peaks_mz_ms2': [],        # Liste des m/z
    'peaks_intensities_ms2': [],# Intensit√©s correspondantes
    'ms2_similarity_score': [], # Score de similarit√©
    'confidence_level': []      # Niveau de confiance mis √† jour
}
```


## 5. Utilisation üìä

### 5.1. Pipeline Compl√®te

Pour utiliser la pipeline, il suffit de suivre ces √©tapes :

**1. Pr√©paration des Donn√©es**

Placez vos fichiers dans les dossiers appropri√©s :
```
data/
‚îú‚îÄ‚îÄ input/
‚îÇ   ‚îú‚îÄ‚îÄ samples/          # Vos √©chantillons (.parquet)
‚îÇ   ‚îú‚îÄ‚îÄ blanks/          # Vos blancs
‚îÇ   ‚îú‚îÄ‚îÄ calibration/     # Fichier de calibration CCS
‚îÇ   ‚îî‚îÄ‚îÄ databases/       # Base NORMAN
```

**2. Lancement de la Pipeline**

Depuis le terminal, dans le dossier du projet :
```bash
# Activation de l'environnement
conda activate ms_pipeline

# Lancement de la pipeline
python main.py
```

**3. Options Disponibles**
```bash
# Aide sur les options
python main.py --help

# Sp√©cifier un dossier d'entr√©e diff√©rent
python main.py --input_dir "chemin/vers/donn√©es"

# Sp√©cifier un dossier de sortie
python main.py --output_dir "chemin/vers/sortie"
```

**4. Suivi du Traitement**

La pipeline affiche sa progression :
```
üöÄ D√©marrage du traitement...
   
üìä Traitement des blancs...
‚úì Blanc 1 trait√©
‚úì Blanc 2 trait√©

üîç Traitement des √©chantillons...
‚úì √âchantillon 1 (3 r√©plicats)
‚úì √âchantillon 2 (3 r√©plicats)
...

‚ú® Traitement termin√© !
```

**5. R√©sultats**

Les r√©sultats sont automatiquement sauvegard√©s dans `data/output/` :
- `feature_matrix.parquet` : Matrice d'intensit√©s
- `feature_matrix.csv` : Version CSV de la matrice
- `features_complete.parquet` : Donn√©es compl√®tes avec identifications
- `features_complete.csv` : Version CSV des identifications


### 5.2. Utilisation Modulaire

La pipeline peut √™tre utilis√©e de mani√®re modulaire pour r√©pondre √† diff√©rents besoins sp√©cifiques.

**1. Analyse d'un Seul √âchantillon**
```python
from scripts.processing.peak_detection import PeakDetector
from scripts.processing.ccs_calibration import CCSCalibrator

# D√©tection des pics uniquement
detector = PeakDetector()
peaks = detector.process_sample(data)

# Avec calcul des CCS
calibrator = CCSCalibrator("calibration.csv")
peaks_with_ccs = calibrator.calculate_ccs(peaks)
```

**2. Traitement de R√©plicats sans Blancs**
```python
from scripts.processing.replicate_processing import ReplicateProcessor

processor = ReplicateProcessor()

# D√©finir les fichiers r√©plicats
replicate_files = [
    "sample_replicate_1.parquet",
    "sample_replicate_2.parquet",
    "sample_replicate_3.parquet"
]

# Traitement des r√©plicats uniquement
results = processor.process_sample_with_replicates(
    sample_name="mon_echantillon",
    replicate_files=replicate_files,
    output_dir="output"
)
```

**3. Identification Cibl√©e**
```python
from scripts.processing.identification import CompoundIdentifier

identifier = CompoundIdentifier()

# Identification avec param√®tres personnalis√©s
matches = identifier.identify_compounds(
    peaks_df=peaks,
    output_dir="output",
    mz_tolerance=3,  # ppm
    rt_tolerance=0.5 # min
)

# Filtrer par niveau de confiance
high_confidence = matches[matches['confidence_level'] <= 2]
```

**4. Analyse MS2 Sp√©cifique**
```python
from scripts.processing.ms2_extraction import MS2Extractor

extractor = MS2Extractor()

# Extraction pour des coordonn√©es sp√©cifiques
spectrum = extractor.extract_ms2_spectrum(
    ms2_data=data,
    rt=4.5,    # temps de r√©tention cible
    dt=35.2    # temps de d√©rive cible
)
```

**5. Cr√©ation de Matrices Personnalis√©es**
```python
from scripts.processing.feature_matrix import FeatureProcessor

processor = FeatureProcessor()

# Cr√©ation d'une matrice pour un sous-ensemble
matrix, features = processor.align_features_across_samples(
    samples_dir="samples_subset",
    min_samples=2,        # pr√©sent dans au moins 2 √©chantillons
    intensity_threshold=500
)
```

**6. Workflow pour √âtude de R√©p√©tabilit√©**
```python
# Analyse de la variabilit√© entre r√©plicats
replicates = processor.process_replicates(replicate_files)
stats = {
    'rsd_mz': [],      # RSD des masses
    'rsd_rt': [],      # RSD des temps de r√©tention
    'rsd_intensity': [] # RSD des intensit√©s
}

for peak_group in replicates.groupby('cluster'):
    stats['rsd_mz'].append(peak_group['mz'].std() / peak_group['mz'].mean() * 100)
    # etc...
```


## 6. R√©sultats et Visualisation üìà

### 6.1. Formats de Sortie

La pipeline g√©n√®re plusieurs fichiers de sortie organis√©s de mani√®re structur√©e.

**Structure des Dossiers de Sortie**
```
data/output/
‚îú‚îÄ‚îÄ feature_matrix.parquet     # Matrice principale des features
‚îú‚îÄ‚îÄ feature_matrix.csv        
‚îú‚îÄ‚îÄ features_complete.parquet  # Donn√©es avec identifications
‚îî‚îÄ‚îÄ features_complete.csv     
```

**1. Matrice des Features**
- Format : CSV et Parquet
- Structure :
```python
# feature_matrix.csv
              F001_mz123.45  F002_mz456.78  F003_mz789.01
Sample_1         1234.56       0.00           789.01
Sample_2         1567.89       456.78         0.00
Sample_3         1432.12       567.89         654.32
```

**2. Liste des Identifications**
- Format : CSV et Parquet
- Contenu :
```python
# features_complete.csv
Colonnes principales :
- feature_id       # Identifiant unique
- mz              # Masse mesur√©e
- rt              # Temps de r√©tention
- drift_time      # Temps de d√©rive
- ccs             # Valeur CCS calcul√©e
- match_name      # Nom du compos√© identifi√©
- formula         # Formule mol√©culaire
- score           # Score global
- confidence      # Niveau de confiance
```

**Exemple de R√©sultats**
| Compos√© | m/z mesur√© | m/z th√©orique | Erreur (ppm) | CCS | RT (min) | Score MS2 | Formule | Adduit |
|---------|------------|---------------|--------------|-----|-----------|-----------|----------|---------|
| Caf√©ine | 195.0879 | 195.0882 | -1.11 | 143.97 | 4.01 | 0.89 | C8H10N4O2 | [M+H]+ |
| Parac√©tamol | 152.0706 | 152.0712 | -3.94 | 131.45 | 3.22 | 0.92 | C8H9NO2 | [M+H]+ |
| Ibuprof√®ne | 207.1378 | 207.1380 | -0.96 | 152.88 | 8.45 | 0.78 | C13H18O2 | [M+H]+ |

**Formats Disponibles**
- `.parquet` : Format optimis√© pour l'analyse ult√©rieure
- `.csv` : Format lisible et compatible avec Excel
- Tous les fichiers incluent des en-t√™tes explicites
- Les valeurs manquantes sont repr√©sent√©es par 0 ou NA selon le contexte

**Acc√®s aux R√©sultats**
```python
# Lecture des r√©sultats
import pandas as pd

# Matrice des features
matrix = pd.read_parquet("data/output/feature_matrix.parquet")

# Identifications compl√®tes
identifications = pd.read_parquet("data/output/features_complete.parquet")
```

**Analyse des R√©sultats**

La matrice de features (`feature_matrix.csv/parquet`) contient :
- Les intensit√©s de chaque feature dans tous les √©chantillons
- Format : √©chantillons en lignes, features en colonnes
- Nomenclature : `FXXX_mzYYY.YYYY` o√π :
  - XXX : num√©ro unique de la feature
  - YYY.YYYY : masse exacte mesur√©e

Dans `features_complete.csv/parquet`, vous trouverez :
- Toutes les features identifi√©es
- Param√®tres analytiques (m/z, RT, DT, CCS)
- Informations d'identification (nom, formule, score)


## 7. Licence ‚öñÔ∏è

### 7.1. Informations de Licence

Ce projet est sous licence [Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/).

**Droits Accord√©s**
- ‚úÖ Partager : copier et redistribuer le mat√©riel sous n'importe quel format
- ‚úÖ Adapter : remixer, transformer et cr√©er √† partir du mat√©riel
- ‚úÖ Le titulaire des droits ne peut pas r√©voquer ces droits tant que vous suivez les termes de la licence

### 7.2. Conditions d'Utilisation

**√Ä Faire**
- Cr√©dit : Vous devez donner le cr√©dit appropri√©, fournir un lien vers la licence et indiquer si des modifications ont √©t√© apport√©es
- Usage Non Commercial : Vous ne pouvez pas utiliser le mat√©riel √† des fins commerciales
- M√™me Licence : Si vous remixez, transformez ou cr√©ez √† partir du mat√©riel, vous devez distribuer vos contributions sous la m√™me licence

**Restrictions**
- ‚ùå Usage Commercial : Cette licence interdit express√©ment l'utilisation commerciale
- ‚ùå Garanties : Pas de garanties fournies avec la licence

### 7.3. Citation

Pour citer ce projet dans une publication acad√©mique, veuillez utiliser :

```bibtex
@software{pipeline_identification_ms,
    title = {Pipeline d'Identification MS},
    year = {2024},
    author = {Sade, Julien},
    url = {https://github.com/pipeline_identification_ms},
    version = {1.0.0},
    institution = {Leesu},
    note = {Pipeline pour l'identification de compos√©s par spectrom√©trie de masse}
}
```

Pour une citation dans le texte :
> Sade, J. (2024). Pipeline d'Identification MS,https://github.com/pipeline_identification_ms

**Contact**

Pour toute question concernant l'utilisation ou la licence :
- ‚úâÔ∏è julien.sade@u-pec.fr
- üåê GitHub Issues : https://github.com/pipeline_identification_ms/issues

