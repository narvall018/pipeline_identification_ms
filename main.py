#main.py
#-*- coding:utf-8 -*-

import logging
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import numpy as np
import logging
import warnings
import pandas as pd
from pathlib import Path
from typing import Union, List, Dict, Tuple
from queue import Queue
from io import StringIO
import sys
from contextlib import redirect_stdout
import time
from tqdm import tqdm
import matplotlib.pyplot as plt 
from scripts.config.config import Config
from scripts.utils.io_handlers import read_parquet_data, save_peaks
from scripts.processing.peak_detection import prepare_data, detect_peaks, cluster_peaks
from scripts.processing.ccs_calibration import CCSCalibrator
from scripts.processing.identification import CompoundIdentifier
from scripts.visualization.plotting import (
    plot_unique_molecules_per_sample,
    plot_level1_molecules_per_sample,
    plot_sample_similarity_heatmap,
    plot_sample_similarity_heatmap_by_confidence,
    plot_level1_molecule_distribution_bubble,
    analyze_sample_clusters,
    plot_cluster_statistics,
    analyze_and_save_clusters,
    plot_tics_interactive
)
from scripts.utils.replicate_handling import group_replicates
from scripts.processing.replicate_processing import process_sample_with_replicates
from scripts.processing.blank_processing import (
    process_blank_with_replicates,
    subtract_blank_peaks
)
from scripts.processing.feature_matrix import create_feature_matrix



# Suppression des warnings pandas
warnings.filterwarnings('ignore')
pd.options.mode.chained_assignment = None


# Initialiser le logger
logger = logging.getLogger(__name__)

def setup_logging() -> None:
    """Configure le systÃ¨me de logging."""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    logging.basicConfig(
        filename=log_dir / "peak_detection.log",
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        force=True
    )

class CaptureOutput:
    """Capture la sortie standard pour chaque processus."""
    def __init__(self):
        self.output = StringIO()
        self.stdout = sys.stdout
        
    def __enter__(self):
        sys.stdout = self.output
        return self.output
        
    def __exit__(self, *args):
        sys.stdout = self.stdout

class SampleResult:
    """Stocke les rÃ©sultats du traitement d'un Ã©chantillon."""
    def __init__(self, name: str, peaks_df: pd.DataFrame, processing_time: float, logs: str):
        self.name = name
        self.peaks_df = peaks_df
        self.processing_time = processing_time
        self.logs = logs
        self.success = not peaks_df.empty
        
        # Extraire les statistiques des logs
        self.initial_peaks = 0
        self.after_clustering = 0
        self.after_blank = 0
        self.final_peaks = len(peaks_df) if not peaks_df.empty else 0
        
        for line in logs.split('\n'):
            if "Pics initiaux:" in line:
                try:
                    self.initial_peaks = int(line.split(": ")[1])
                except:
                    pass
            elif "aprÃ¨s clustering:" in line and "aprÃ¨s clustering: {len" not in line:
                try:
                    self.after_clustering = int(line.split(": ")[1])
                except:
                    pass
            elif "pics aprÃ¨s soustraction du blank" in line:
                try:
                    self.after_blank = int(line.split(" ")[3])
                except:
                    pass


def process_blank_files(blank_files: List[Path]) -> pd.DataFrame:
    """Traite tous les fichiers blanks."""
    blank_peaks = pd.DataFrame()
    
    if blank_files:
        print(f"   âœ“ {len(blank_files)} fichier(s) blank trouvÃ©(s):")
        for blank_file in blank_files:
            print(f"      - {blank_file.name}")
            
        for blank_file in blank_files:
            blank_peaks_group = process_blank_with_replicates(
                blank_file.stem,
                [blank_file],
                Path("data/intermediate/blanks")
            )
            if not blank_peaks_group.empty:
                blank_peaks = pd.concat([blank_peaks, blank_peaks_group])
        print(f"   âœ“ {len(blank_peaks)} pics de blank dÃ©tectÃ©s")
    else:
        print("   â„¹ï¸ Aucun blank trouvÃ© dans data/input/blanks/")
        
    return blank_peaks

# DÃ©finir les fonctions de visualisation au niveau global
def generate_molecules_per_sample(output_dir: Path):
    fig = plot_unique_molecules_per_sample(output_dir)
    fig.savefig(output_dir / "molecules_per_sample.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_level1_molecules(output_dir: Path):
    fig = plot_level1_molecules_per_sample(output_dir)
    fig.savefig(output_dir / "level1_molecules_per_sample.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_bubble_plot(output_dir: Path):
    fig = plot_level1_molecule_distribution_bubble(output_dir)
    fig.savefig(output_dir / "level1_molecule_distribution_bubble.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_similarity_heatmap(output_dir: Path):
    fig = plot_sample_similarity_heatmap(output_dir)
    fig.savefig(output_dir / "sample_similarity_heatmap_all.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_level1_heatmap(output_dir: Path):
    fig = plot_sample_similarity_heatmap_by_confidence(
        output_dir, 
        confidence_levels=[1],
        title_suffix=" - Niveau 1"
    )
    fig.savefig(output_dir / "sample_similarity_heatmap_level1.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_level12_heatmap(output_dir: Path):
    fig = plot_sample_similarity_heatmap_by_confidence(
        output_dir, 
        confidence_levels=[1, 2],
        title_suffix=" - Niveaux 1 et 2"
    )
    fig.savefig(output_dir / "sample_similarity_heatmap_level12.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_level123_heatmap(output_dir: Path):
    fig = plot_sample_similarity_heatmap_by_confidence(
        output_dir, 
        confidence_levels=[1, 2, 3],
        title_suffix=" - Niveaux 1, 2 et 3"
    )
    fig.savefig(output_dir / "sample_similarity_heatmap_level123.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_tics(output_dir: Path):
    plot_tics_interactive(Path("data/input/samples"), output_dir)

def generate_visualizations(output_dir: Path) -> None:
    """GÃ©nÃ¨re toutes les visualisations de la pipeline en parallÃ¨le."""
    try:
        print("\nğŸ“Š GÃ©nÃ©ration des visualisations en parallÃ¨le...")
        output_dir.mkdir(exist_ok=True)
        
        # Les fichiers d'identifications sont dans output_dir/feature_matrix/
        identifications_file = output_dir / "feature_matrix" / "feature_identifications.parquet"
        if not identifications_file.exists():
            raise FileNotFoundError(f"Fichier d'identifications non trouvÃ©: {identifications_file}")

        # Liste des tÃ¢ches Ã  exÃ©cuter avec leurs arguments
        tasks = [
            (generate_molecules_per_sample, output_dir),
            (generate_level1_molecules, output_dir),
            (generate_bubble_plot, output_dir),
            (generate_similarity_heatmap, output_dir),
            (generate_level1_heatmap, output_dir),
            (generate_level12_heatmap, output_dir),
            (generate_level123_heatmap, output_dir),
            (generate_tics, output_dir)
        ]

        # ExÃ©cution parallÃ¨le des tÃ¢ches
        max_workers = min(mp.cpu_count(), len(tasks))
        futures_to_task = {}
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # Soumettre toutes les tÃ¢ches
            for func, arg in tasks:
                future = executor.submit(func, arg)
                futures_to_task[future] = func.__name__
            
            # Attendre leur complÃ©tion avec une barre de progression
            with tqdm(total=len(futures_to_task), desc="GÃ©nÃ©ration des visualisations") as pbar:
                for future in as_completed(futures_to_task):
                    task_name = futures_to_task[future]
                    try:
                        future.result()
                    except Exception as e:
                        print(f"Erreur lors de la gÃ©nÃ©ration de {task_name}: {str(e)}")
                    pbar.update(1)

        print(f"   âœ“ Visualisations sauvegardÃ©es dans {output_dir}")

    except Exception as e:
        print(f"âŒ Erreur lors de la crÃ©ation des visualisations: {str(e)}")
        raise

def process_single_sample(
    args: Tuple[str, List[Path], pd.DataFrame, CCSCalibrator, Path]
) -> Tuple[str, SampleResult]:
    """Traite un seul Ã©chantillon en capturant sa sortie."""
    base_name, replicates, blank_peaks, calibrator, output_base_dir = args
    start_time = time.time()
    
    with CaptureOutput() as output:
        try:
            # 1. Traitement des rÃ©plicats ğŸ“Š
            common_peaks = process_sample_with_replicates(
                base_name,
                replicates,
                output_base_dir
            )
            
            if common_peaks.empty:
                print(f"âœ— Pas de pics trouvÃ©s pour {base_name}")
                return base_name, SampleResult(
                    base_name, pd.DataFrame(), 
                    time.time() - start_time, 
                    output.getvalue()
                )
            
            # 2. Soustraction du blank 
            if not blank_peaks.empty:
                clean_peaks = subtract_blank_peaks(common_peaks, blank_peaks)
            else:
                clean_peaks = common_peaks
                
            if clean_peaks.empty:
                print(f"âœ— Pas de pics aprÃ¨s soustraction du blank pour {base_name}")
                return base_name, SampleResult(
                    base_name, pd.DataFrame(), 
                    time.time() - start_time, 
                    output.getvalue()
                )
            
            # 3. Calibration CCS 
            peaks_with_ccs = calibrator.calculate_ccs(clean_peaks)
            
            # Mise Ã  jour des statistiques
            peaks_stats = {
                'initial': len(common_peaks),
                'after_clustering': len(clean_peaks),  # Correspond aux pics aprÃ¨s clustering
                'after_blank': len(clean_peaks),  # Nombre identique aux pics finaux
                'final': len(peaks_with_ccs)
            }
            
            if not peaks_with_ccs.empty:
                print(f"âœ“ CCS calculÃ©es pour {len(peaks_with_ccs)} pics")
                print(f"âœ“ Plage de CCS: {peaks_with_ccs['CCS'].min():.2f} - {peaks_with_ccs['CCS'].max():.2f} Ã…Â²")
                print(f"âœ“ CCS moyenne: {peaks_with_ccs['CCS'].mean():.2f} Ã…Â²")
                
                # Sauvegarde
                output_dir = output_base_dir / base_name / "ms1"
                output_dir.mkdir(parents=True, exist_ok=True)
                output_file = output_dir / "common_peaks.parquet"
                peaks_with_ccs.to_parquet(output_file)
                
            return base_name, SampleResult(
                base_name, peaks_with_ccs,
                time.time() - start_time,
                output.getvalue()
            )
            
        except Exception as e:
            print(f"âŒ Erreur lors du traitement de {base_name}: {str(e)}")
            return base_name, SampleResult(
                base_name, pd.DataFrame(),
                time.time() - start_time,
                output.getvalue()
            )

def process_samples_parallel(
    replicate_groups: Dict[str, List[Path]],
    blank_peaks: pd.DataFrame,
    calibrator: CCSCalibrator,
    output_base_dir: Path,
    max_workers: int = None
) -> Dict[str, SampleResult]:
    """Traite tous les Ã©chantillons en parallÃ¨le avec barre de progression."""
    if max_workers is None:
        max_workers = mp.cpu_count()
    
    total_samples = len(replicate_groups)    
    print("\n" + "="*80)
    print(f"TRAITEMENT DES Ã‰CHANTILLONS ({total_samples} Ã©chantillons)")
    print("="*80)
    
    total_start_time = time.time()
    
    process_args = [(base_name, replicates, blank_peaks, calibrator, output_base_dir)
                   for base_name, replicates in replicate_groups.items()]
    
    results = {}
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        future_to_name = {
            executor.submit(process_single_sample, args): args[0]
            for args in process_args
        }
        
        with tqdm(total=len(future_to_name), unit="Ã©chantillon") as pbar:
            for future in as_completed(future_to_name):
                base_name, result = future.result()
                results[base_name] = result
                pbar.update(1)
    
    total_time = time.time() - total_start_time
    success_count = sum(1 for r in results.values() if r.success)
    failed_count = len(results) - success_count
    
    print("\n" + "="*80)
    print("RÃ‰CAPITULATIF")
    print("="*80)
    
    print(f"\nTemps total: {total_time:.2f} secondes")
    print(f"Ã‰chantillons traitÃ©s: {success_count}/{len(results)}")
    if failed_count > 0:
        print(f"âŒ Ã‰chantillons en Ã©chec: {failed_count}")
        
    print("\nDÃ©tails par Ã©chantillon:")
    for name, result in results.items():
        status = "âœ“" if result.success else "âœ—"
        print(f"\n{status} {name}")
        print(f"  â€¢ Temps de traitement: {result.processing_time:.2f}s")
        print(f"  â€¢ Pics initiaux: {result.initial_peaks}")
        if result.success:
            print(f"  â€¢ Pics aprÃ¨s clustering: {result.after_clustering}")
            print(f"  â€¢ Pics aprÃ¨s blank: {result.final_peaks}")
            print(f"  â€¢ Pics finaux: {result.final_peaks}")
            if result.final_peaks > 0:
                ccs_values = result.peaks_df['CCS']
                print(f"  â€¢ CCS min-max: {ccs_values.min():.2f} - {ccs_values.max():.2f} Ã…Â²")
                print(f"  â€¢ CCS moyenne: {ccs_values.mean():.2f} Ã…Â²")
    
    stats_df = pd.DataFrame([{
        'Ã‰chantillon': r.name,
        'Temps (s)': r.processing_time,
        'Pics initiaux': r.initial_peaks,
        'Pics aprÃ¨s clustering': r.after_clustering,
        'Pics aprÃ¨s blank': r.final_peaks,
        'Pics finaux': r.final_peaks,
        'Statut': 'SuccÃ¨s' if r.success else 'Ã‰chec'
    } for r in results.values()])
    
    stats_file = output_base_dir / "processing_statistics.csv"
    stats_df.to_csv(stats_file, index=False)
    print(f"\nStatistiques sauvegardÃ©es dans {stats_file}")
    
    return results


def main() -> None:
    """Point d'entrÃ©e principal de la pipeline."""
    setup_logging()
    print("\nğŸš€ DÃ‰MARRAGE DE LA PIPELINE D'ANALYSE")
    print("=" * 80)

    try:
        # 1. Chargement des donnÃ©es de calibration CCS
        print("\nğŸ“ˆ Chargement des donnÃ©es de calibration CCS...")
        calibration_file = Path("data/input/calibration/CCS_calibration_data.csv")
        if not calibration_file.exists():
            raise FileNotFoundError(f"Fichier de calibration non trouvÃ© : {calibration_file}")
        calibrator = CCSCalibrator(calibration_file)
        print("   âœ“ DonnÃ©es de calibration chargÃ©es avec succÃ¨s")

        # 2. Initialisation de l'identificateur
        print("\nğŸ“š Initialisation de l'identification...")
        identifier = CompoundIdentifier()
        print("   âœ“ Base de donnÃ©es chargÃ©e avec succÃ¨s")

        # 3. Recherche des fichiers
        print("\nğŸ“ Recherche des blanks...")
        blank_dir = Path("data/input/blanks")
        blank_files = list(blank_dir.glob("*.parquet"))
        
        print("\nğŸ“ Recherche des fichiers d'Ã©chantillons...")
        samples_dir = Path(Config.INPUT_SAMPLES)
        sample_files = list(samples_dir.glob("*.parquet"))

        if not sample_files:
            raise ValueError("Aucun fichier d'Ã©chantillon trouvÃ©.")
            
        replicate_groups = group_replicates(sample_files)
        print(f"   âœ“ {len(replicate_groups)} Ã©chantillons trouvÃ©s:")
        for base_name, replicates in replicate_groups.items():
            print(f"      - {base_name}: {len(replicates)} rÃ©plicat(s)")

        # 4. Traitement des blanks (sÃ©quentiel)
        blank_peaks = process_blank_files(blank_files)

        # 5. Traitement parallÃ¨le des Ã©chantillons
        results = process_samples_parallel(
            replicate_groups,
            blank_peaks,
            calibrator,
            Path("data/intermediate/samples")
        )
        
        # VÃ©rifier si des Ã©chantillons ont Ã©tÃ© traitÃ©s avec succÃ¨s
        if not any(r.success for r in results.values()):
            raise Exception("Aucun Ã©chantillon n'a Ã©tÃ© traitÃ© avec succÃ¨s")

        # 6. Feature Matrix et identification
        print("\nğŸ“Š CrÃ©ation de la matrice des features...")
        create_feature_matrix(
            input_dir=Path("data/intermediate/samples"),
            output_dir=Path("output/feature_matrix"),
            identifier=identifier
        )

        # 7. Visualisations
        output_dir = Path("output")
        generate_visualizations(output_dir)
        
        print("\nğŸ“Š Analyse des similaritÃ©s entre Ã©chantillons...")
        analyze_and_save_clusters(output_dir)

        print("\nâœ… TRAITEMENT TERMINÃ‰ AVEC SUCCÃˆS")
        print("=" * 80)

    except Exception as e:
        print("\nâŒ ERREUR DANS LA PIPELINE")
        logger.error(f"Erreur pipeline : {str(e)}")
        raise

if __name__ == "__main__":
    main()