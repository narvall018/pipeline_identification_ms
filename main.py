#main.py
#-*- coding:utf-8 -*--

import gc 
import logging
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import numpy as np
import logging
import warnings
import pandas as pd
from pathlib import Path
from typing import Union, List, Dict, Tuple
from queue import Queue
from io import StringIO
import sys
from contextlib import redirect_stdout
import time
from tqdm import tqdm
import matplotlib.pyplot as plt 
from scripts.config.config import Config
from scripts.utils.io_handlers import read_parquet_data, save_peaks
from scripts.processing.peak_detection import prepare_data, detect_peaks, cluster_peaks
from scripts.processing.ccs_calibration import CCSCalibrator
from scripts.processing.identification import CompoundIdentifier
from scripts.visualization.plotting import (
    plot_unique_molecules_per_sample,
    plot_level1_molecules_per_sample,
    plot_sample_similarity_heatmap,
    plot_sample_similarity_heatmap_by_confidence,
    plot_level1_molecule_distribution_bubble,
    analyze_sample_clusters,
    plot_cluster_statistics,
    analyze_and_save_clusters,
    plot_tics_interactive,
    analyze_categories
)
from scripts.utils.replicate_handling import group_replicates
from scripts.processing.replicate_processing import process_sample_with_replicates
from scripts.processing.blank_processing import (
    process_blank_with_replicates,
    subtract_blank_peaks
)
from scripts.processing.feature_matrix import create_feature_matrix



# Suppression des warnings pandas
warnings.filterwarnings('ignore')
pd.options.mode.chained_assignment = None


# Initialiser le logger
logger = logging.getLogger(__name__)

def setup_logging() -> None:
    """Configure le syst√®me de logging."""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    logging.basicConfig(
        filename=log_dir / "peak_detection.log",
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        force=True
    )

class CaptureOutput:
    """Capture la sortie standard pour chaque processus."""
    def __init__(self):
        self.output = StringIO()
        self.stdout = sys.stdout
        
    def __enter__(self):
        sys.stdout = self.output
        return self.output
        
    def __exit__(self, *args):
        sys.stdout = self.stdout

class SampleResult:
    """Stocke les r√©sultats du traitement d'un √©chantillon."""
    def __init__(self, name: str, peaks_df: pd.DataFrame, processing_time: float, logs: str):
        self.name = name
        self.peaks_df = peaks_df
        self.processing_time = processing_time
        self.logs = logs
        self.success = not peaks_df.empty
        
        # Extraire les statistiques des logs
        self.initial_peaks = 0
        self.after_clustering = 0
        self.after_blank = len(peaks_df) if not peaks_df.empty else 0  # Nombre de pics finaux
        self.final_peaks = len(peaks_df) if not peaks_df.empty else 0
        
        for line in logs.split('\n'):
            if "Pics initiaux:" in line:
                try:
                    self.initial_peaks = int(line.split(": ")[1])
                except:
                    pass
            elif "apr√®s clustering:" in line and "apr√®s clustering: {len" not in line:
                try:
                    self.after_clustering = int(line.split(": ")[1])
                except:
                    pass
            elif "Pics apr√®s soustraction" in line:
                try:
                    self.after_blank = int(line.split(": ")[1])
                except:
                    pass


def process_blank_files(blank_files: List[Path]) -> pd.DataFrame:
    """Traite tous les fichiers blanks."""
    blank_peaks = pd.DataFrame()
    
    # Message concernant la pr√©sence ou l'absence de blanks
    if blank_files:
        print(f"   ‚úì {len(blank_files)} fichier(s) blank trouv√©(s):")
        for blank_file in blank_files:
            print(f"      - {blank_file.name}")
    else:
        print("   ‚ÑπÔ∏è Aucun blank trouv√© dans data/input/blanks/")
            
    if blank_files:
        for blank_file in blank_files:
            blank_peaks_group = process_blank_with_replicates(
                blank_file.stem,
                [blank_file],
                Path("data/intermediate/blanks")
            )
            if not blank_peaks_group.empty:
                blank_peaks = pd.concat([blank_peaks, blank_peaks_group])
        if not blank_peaks.empty:
            print(f"   ‚úì {len(blank_peaks)} pics de blank d√©tect√©s")
            
    return blank_peaks

def generate_molecules_per_sample(output_dir: Path):
    fig = plot_unique_molecules_per_sample(output_dir)
    fig.savefig(output_dir / "molecules_per_sample.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_level1_molecules(output_dir: Path):
    fig = plot_level1_molecules_per_sample(output_dir)
    fig.savefig(output_dir / "level1_molecules_per_sample.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_bubble_plot(output_dir: Path):
    fig = plot_level1_molecule_distribution_bubble(output_dir)
    fig.savefig(output_dir / "level1_molecule_distribution_bubble.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_similarity_heatmap(output_dir: Path):
    fig = plot_sample_similarity_heatmap(output_dir)
    fig.savefig(output_dir / "sample_similarity_heatmap_all.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_level1_heatmap(output_dir: Path):
    fig = plot_sample_similarity_heatmap_by_confidence(
        output_dir, 
        confidence_levels=[1],
        title_suffix=" - Niveau 1"
    )
    fig.savefig(output_dir / "sample_similarity_heatmap_level1.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_level12_heatmap(output_dir: Path):
    fig = plot_sample_similarity_heatmap_by_confidence(
        output_dir, 
        confidence_levels=[1, 2],
        title_suffix=" - Niveaux 1 et 2"
    )
    fig.savefig(output_dir / "sample_similarity_heatmap_level12.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_level123_heatmap(output_dir: Path):
    fig = plot_sample_similarity_heatmap_by_confidence(
        output_dir, 
        confidence_levels=[1, 2, 3],
        title_suffix=" - Niveaux 1, 2 et 3"
    )
    fig.savefig(output_dir / "sample_similarity_heatmap_level123.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_tics(output_dir: Path):
    plot_tics_interactive(Path("data/input/samples"), output_dir)


def generate_visualizations(output_dir: Path) -> None:
    """G√©n√®re toutes les visualisations de la pipeline en parall√®le."""
    try:
        print("\nüìä G√©n√©ration des visualisations...")
        output_dir.mkdir(exist_ok=True)
        
        # V√©rifier l'existence du fichier d'identifications dans le sous-dossier feature_matrix
        identifications_file = output_dir / "feature_matrix" / "features_complete.parquet"
        if not identifications_file.exists():
            raise FileNotFoundError(f"Fichier d'identifications non trouv√©: {identifications_file}")

        # Liste des t√¢ches √† ex√©cuter avec leurs arguments
        tasks = [
            (generate_molecules_per_sample, output_dir),
            (generate_level1_molecules, output_dir),
            (generate_bubble_plot, output_dir),
            (generate_similarity_heatmap, output_dir),
            (generate_level1_heatmap, output_dir),
            (generate_level12_heatmap, output_dir),
            (generate_level123_heatmap, output_dir)
            #,(generate_tics, output_dir)
        ]

        # Ex√©cution parall√®le des t√¢ches
        max_workers = min(mp.cpu_count(), len(tasks))

        futures_to_task = {}
        with ProcessPoolExecutor(max_workers=2) as executor:
            # Soumettre toutes les t√¢ches
            for func, arg in tasks:
                future = executor.submit(func, arg)
                futures_to_task[future] = func.__name__
            
            # Attendre leur compl√©tion avec une barre de progression
            with tqdm(total=len(futures_to_task), desc="G√©n√©ration des visualisations") as pbar:
                for future in as_completed(futures_to_task):
                    task_name = futures_to_task[future]
                    try:
                        future.result()
                    except Exception as e:
                        print(f"Erreur lors de la g√©n√©ration de {task_name}: {str(e)}")
                    pbar.update(1)

        print(f"   ‚úì Visualisations sauvegard√©es dans {output_dir}")

    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation des visualisations: {str(e)}")
        raise


def process_single_sample(
    args: Tuple[str, List[Path], pd.DataFrame, CCSCalibrator, Path]
) -> Tuple[str, SampleResult]:
    """Traite un seul √©chantillon en capturant sa sortie."""
    base_name, replicates, blank_peaks, calibrator, output_base_dir = args
    start_time = time.time()
    
    with CaptureOutput() as output:
        try:
            # 1. Traitement des r√©plicats üìä
            common_peaks = process_sample_with_replicates(
                base_name,
                replicates,
                output_base_dir
            )
            
            if common_peaks.empty:
                print(f"‚úó Pas de pics trouv√©s pour {base_name}")
                return base_name, SampleResult(
                    base_name, pd.DataFrame(), 
                    time.time() - start_time, 
                    output.getvalue()
                )
            
            # 2. Soustraction du blank 
            if not blank_peaks.empty:
                clean_peaks = subtract_blank_peaks(common_peaks, blank_peaks)
            else:
                clean_peaks = common_peaks
                
            if clean_peaks.empty:
                print(f"‚úó Pas de pics apr√®s soustraction du blank pour {base_name}")
                return base_name, SampleResult(
                    base_name, pd.DataFrame(), 
                    time.time() - start_time, 
                    output.getvalue()
                )
            
            # 3. Calibration CCS 
            peaks_with_ccs = calibrator.calculate_ccs(clean_peaks)
            
            # Mise √† jour des statistiques
            peaks_stats = {
                'initial': len(common_peaks),
                'after_clustering': len(clean_peaks),  # Correspond aux pics apr√®s clustering
                'after_blank': len(clean_peaks),  # Nombre identique aux pics finaux
                'final': len(peaks_with_ccs)
            }
            
            if not peaks_with_ccs.empty:
                print(f"‚úì CCS calcul√©es pour {len(peaks_with_ccs)} pics")
                print(f"‚úì Plage de CCS: {peaks_with_ccs['CCS'].min():.2f} - {peaks_with_ccs['CCS'].max():.2f} √Ö¬≤")
                print(f"‚úì CCS moyenne: {peaks_with_ccs['CCS'].mean():.2f} √Ö¬≤")
                
                # Sauvegarde
                output_dir = output_base_dir / base_name / "ms1"
                output_dir.mkdir(parents=True, exist_ok=True)
                output_file = output_dir / "common_peaks.parquet"
                peaks_with_ccs.to_parquet(output_file)
                
            return base_name, SampleResult(
                base_name, peaks_with_ccs,
                time.time() - start_time,
                output.getvalue()
            )
            
        except Exception as e:
            print(f"‚ùå Erreur lors du traitement de {base_name}: {str(e)}")
            return base_name, SampleResult(
                base_name, pd.DataFrame(),
                time.time() - start_time,
                output.getvalue()
            )

def process_samples_parallel(
    replicate_groups: Dict[str, List[Path]],
    blank_peaks: pd.DataFrame,
    calibrator: CCSCalibrator,
    output_base_dir: Path,
    max_workers: int = None
) -> Dict[str, SampleResult]:
    """Traite tous les √©chantillons en parall√®le avec gestion de la m√©moire."""
    if max_workers is None:
        max_workers = min(2, mp.cpu_count())
    
    total_samples = len(replicate_groups)    
    print("\n" + "="*80)
    print(f"TRAITEMENT DES √âCHANTILLONS ({total_samples} √©chantillons)")
    print("="*80)
    
    total_start_time = time.time()
    results = {}
    
    # Traiter les √©chantillons par petits groupes
    batch_size = 2  # R√©duit √† 2 √©chantillons √† la fois
    sample_items = list(replicate_groups.items())
    
    with tqdm(total=len(sample_items), unit="√©chantillon") as pbar:
        for i in range(0, len(sample_items), batch_size):
            batch_items = sample_items[i:i + batch_size]
            
            process_args = [
                (base_name, replicates, blank_peaks, calibrator, output_base_dir)
                for base_name, replicates in batch_items
            ]
            
            try:
                with ProcessPoolExecutor(max_workers=max_workers) as executor:
                    future_to_name = {
                        executor.submit(process_single_sample, args): args[0]
                        for args in process_args
                    }
                    
                    for future in as_completed(future_to_name):
                        base_name = future_to_name[future]
                        try:
                            _, result = future.result()
                            results[base_name] = result
                        except Exception as e:
                            print(f"\n‚ùå Erreur pour {base_name}: {str(e)}")
                            # Cr√©er un SampleResult vide pour les √©chantillons en erreur
                            results[base_name] = SampleResult(
                                base_name, 
                                pd.DataFrame(), 
                                0.0,
                                f"Erreur: {str(e)}"
                            )
                        finally:
                            pbar.update(1)
                
            except Exception as e:
                print(f"\n‚ùå Erreur dans le batch {i//batch_size + 1}: {str(e)}")
                # G√©rer les √©chantillons non trait√©s dans ce batch
                for name, _ in batch_items:
                    if name not in results:
                        results[name] = SampleResult(
                            name,
                            pd.DataFrame(),
                            0.0,
                            f"Erreur batch: {str(e)}"
                        )
                        pbar.update(1)
            
            # Force la lib√©ration de la m√©moire
            gc.collect()
            
            # Sauvegarde interm√©diaire des statistiques
            stats_df = pd.DataFrame([{
                '√âchantillon': r.name,
                'Temps (s)': r.processing_time,
                'Pics initiaux': r.initial_peaks,
                'Pics apr√®s clustering': r.after_clustering,
                'Pics apr√®s blank': r.after_blank,
                'Pics finaux': r.final_peaks,
                'Statut': 'Succ√®s' if r.success else '√âchec'
            } for r in results.values()])
            
            stats_file = output_base_dir / "processing_statistics.csv"
            stats_df.to_csv(stats_file, index=False)
    
    total_time = time.time() - total_start_time
    success_count = sum(1 for r in results.values() if r.success)
    failed_count = len(results) - success_count
    
    print("\n" + "="*80)
    print("R√âCAPITULATIF")
    print("="*80)
    
    print(f"\nTemps total: {total_time:.2f} secondes")
    print(f"√âchantillons trait√©s: {success_count}/{len(results)}")
    if failed_count > 0:
        print(f"‚ùå √âchantillons en √©chec: {failed_count}")
    
    print("\nD√©tails par √©chantillon:")
    for name, result in results.items():
        status = "‚úì" if result.success else "‚úó"
        print(f"\n{status} {name}")
        print(f"  ‚Ä¢ Temps de traitement: {result.processing_time:.2f}s")
        print(f"  ‚Ä¢ Pics initiaux: {result.initial_peaks}")
        if result.success:
            print(f"  ‚Ä¢ Pics apr√®s clustering: {result.after_clustering}")
            print(f"  ‚Ä¢ Pics apr√®s blank: {result.after_blank}")
            print(f"  ‚Ä¢ Pics finaux: {result.final_peaks}")
            peaks_df = result.peaks_df
            if not peaks_df.empty and 'CCS' in peaks_df.columns:
                ccs_stats = peaks_df['CCS'].agg(['min', 'max', 'mean'])
                print(f"  ‚Ä¢ CCS min-max: {ccs_stats['min']:.2f} - {ccs_stats['max']:.2f} √Ö¬≤")
                print(f"  ‚Ä¢ CCS moyenne: {ccs_stats['mean']:.2f} √Ö¬≤")
        else:
            print(f"  ‚Ä¢ Erreur: {result.logs}")
    
    # Sauvegarde finale des statistiques
    final_stats_df = pd.DataFrame([{
        '√âchantillon': r.name,
        'Temps (s)': r.processing_time,
        'Pics initiaux': r.initial_peaks,
        'Pics apr√®s clustering': r.after_clustering,
        'Pics apr√®s blank': r.after_blank,
        'Pics finaux': r.final_peaks,
        'Statut': 'Succ√®s' if r.success else '√âchec'
    } for r in results.values()])
    
    stats_file = output_base_dir / "processing_statistics.csv"
    final_stats_df.to_csv(stats_file, index=False)
    print(f"\nStatistiques sauvegard√©es dans {stats_file}")
    
    return results


def main() -> None:
    """Point d'entr√©e principal de la pipeline."""
    setup_logging()
    start_time = time.time()
    print("\nüöÄ D√âMARRAGE DE LA PIPELINE D'ANALYSE")
    print("=" * 80)

    try:
        # 1. Chargement des donn√©es de calibration CCS
        print("\nüìà Chargement des donn√©es de calibration CCS...")
        calibration_file = Path("data/input/calibration/CCS_calibration_data.csv")
        if not calibration_file.exists():
            raise FileNotFoundError(f"Fichier de calibration non trouv√© : {calibration_file}")
        calibrator = CCSCalibrator(calibration_file)
        print("   ‚úì Donn√©es de calibration charg√©es avec succ√®s")

        # 2. Initialisation de l'identificateur
        print("\nüìö Initialisation de l'identification...")
        identifier = CompoundIdentifier()
        print("   ‚úì Base de donn√©es charg√©e avec succ√®s")

        # 3. Recherche des fichiers
        print("\nüìÅ Recherche des blanks...")
        blank_dir = Path("data/input/blanks")
        blank_files = list(blank_dir.glob("*.parquet"))
        if blank_files:
            print(f"   ‚úì {len(blank_files)} fichier(s) blank trouv√©(s):")
            for blank_file in blank_files:
                print(f"      - {blank_file.name}")
        else:
            print("   ‚ÑπÔ∏è Aucun blank trouv√© dans data/input/blanks/")

        print("\nüìÅ Recherche des √©chantillons...")
        samples_dir = Path(Config.INPUT_SAMPLES)
        sample_files = list(samples_dir.glob("*.parquet"))

        if not sample_files:
            raise ValueError("Aucun fichier d'√©chantillon trouv√©.")
            
        replicate_groups = group_replicates(sample_files)
        print(f"   ‚úì {len(replicate_groups)} √©chantillons trouv√©s:")
        for base_name, replicates in replicate_groups.items():
            print(f"      - {base_name}: {len(replicates)} r√©plicat(s)")

        print("\n" + "="*80)
        print("TRAITEMENT DES √âCHANTILLONS")
        print("=" * 80)

        # 4. Traitement des blanks
        if blank_files:
            blank_peaks = process_blank_with_replicates(
                blank_files[0].stem,
                blank_files,
                Path("data/intermediate/blanks")
            )
        else:
            blank_peaks = pd.DataFrame()

        # 5. Traitement parall√®le des √©chantillons
        results = process_samples_parallel(
            replicate_groups,
            blank_peaks,
            calibrator,
            Path("data/intermediate/samples")
        )
        
        # V√©rifier si des √©chantillons ont √©t√© trait√©s avec succ√®s
        if not any(r.success for r in results.values()):
            raise Exception("Aucun √©chantillon n'a √©t√© trait√© avec succ√®s")

        print("\n" + "="*80)
        print("ALIGNEMENT DES FEATURES")
        print("="*80)
        
        # 6. Feature Matrix et identification
        print("\nüìä Cr√©ation de la matrice des features...")
        create_feature_matrix(
            input_dir=Path("data/intermediate/samples"),
            output_dir=Path("output/feature_matrix"),
            identifier=identifier
        )

        print("\n" + "="*80)
        print("G√âN√âRATION DES VISUALISATIONS")
        print("="*80)
        
        # 7. Visualisations
        output_dir = Path("output")
        generate_visualizations(output_dir)
        
        print("\n" + "="*80)
        print("ANALYSE DES SIMILARIT√âS")
        print("="*80)
        
        print("\nüìä Analyse des clusters d'√©chantillons...")
        analyze_and_save_clusters(output_dir)

        print("\n" + "="*80)
        print("ANALYSE DES CAT√âGORIES")
        print("="*80)
    
        print("\nüìä Analyse des cat√©gories de mol√©cules...")
        analyze_categories(output_dir)

        total_time = time.time() - start_time
        minutes = int(total_time // 60)
        seconds = int(total_time % 60)
        

        print("\n" + "="*80)
        print(" ‚úÖ FIN DU TRAITEMENT")
        print("="*80)
        print("\n Pipeline d'analyse termin√©e avec succ√®s")
        if minutes > 0:
            print(f"   ‚Ä¢ Temps de calcul total: {minutes} min {seconds} sec")
        else:
            print(f"   ‚Ä¢ Temps de calcul total: {seconds} sec")
        print(f"   ‚Ä¢ {len(replicate_groups)} √©chantillons trait√©s")
        print("=" * 80)

    except Exception as e:
        print("\n‚ùå ERREUR DANS LA PIPELINE")
        logger.error(f"Erreur pipeline : {str(e)}")
        raise

if __name__ == "__main__":
    main()

