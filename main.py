#main.py
#-*- coding:utf-8 -*-

import numpy as np
import logging
import warnings
import pandas as pd
from pathlib import Path
from typing import Union, List
from scripts.config.config import Config
from scripts.utils.io_handlers import read_parquet_data, save_peaks
from scripts.processing.peak_detection import prepare_data, detect_peaks, cluster_peaks
from scripts.processing.ccs_calibration import CCSCalibrator
from scripts.processing.identification import CompoundIdentifier
import matplotlib.pyplot as plt 
from scripts.visualization.plotting import (
    plot_unique_molecules_per_sample,
    plot_level1_molecules_per_sample,
    plot_sample_similarity_heatmap,
    plot_sample_similarity_heatmap_by_confidence,
    plot_level1_molecule_distribution_bubble,analyze_sample_clusters,plot_cluster_statistics,analyze_and_save_clusters)

from scripts.visualization.plotting import plot_tics_interactive
from scripts.utils.replicate_handling import group_replicates
from scripts.processing.replicate_processing import process_sample_with_replicates
from scripts.processing.blank_processing import (
    process_blank_with_replicates,
    subtract_blank_peaks
)
from scripts.processing.feature_matrix import create_feature_matrix

# Suppression des warnings pandas
warnings.filterwarnings('ignore')
pd.options.mode.chained_assignment = None

# Initialiser le logger
logger = logging.getLogger(__name__)

def setup_logging() -> None:
    """Configure le systÃ¨me de logging."""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    logging.basicConfig(
        filename=log_dir / "peak_detection.log",
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        force=True
    )
    logging.info("Logging configurÃ© avec succÃ¨s.")

def process_blank_files(blank_files: List[Path]) -> pd.DataFrame:
    """Traite tous les fichiers blanks."""
    blank_peaks = pd.DataFrame()
    
    if blank_files:
        print(f"   âœ“ {len(blank_files)} fichier(s) blank trouvÃ©(s):")
        for blank_file in blank_files:
            print(f"      - {blank_file.name}")
            
        for blank_file in blank_files:
            blank_peaks_group = process_blank_with_replicates(
                blank_file.stem,
                [blank_file],
                Path("data/intermediate/blanks")
            )
            if not blank_peaks_group.empty:
                blank_peaks = pd.concat([blank_peaks, blank_peaks_group])
        print(f"   âœ“ {len(blank_peaks)} pics de blank dÃ©tectÃ©s")
    else:
        print("   â„¹ï¸ Aucun blank trouvÃ© dans data/input/blanks/")
        
    return blank_peaks

def process_full_sample(
    base_name: str,
    replicates: List[Path],
    blank_peaks: pd.DataFrame,
    calibrator: CCSCalibrator,
    output_base_dir: Path
) -> None:
    """
    Traite un Ã©chantillon complet:
    1. Traitement des rÃ©plicats
    2. Soustraction du blank
    3. Calibration CCS
    """
    print(f"\n{'='*80}")
    print(f"TRAITEMENT DE {base_name} ({len(replicates)} rÃ©plicats)")
    print(f"{'='*80}")
    
    # CrÃ©ation des dossiers de sortie
    output_dir = output_base_dir / base_name / "ms1"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. Traitement des rÃ©plicats et obtention des pics communs
    common_peaks = process_sample_with_replicates(
        base_name,
        replicates,
        Path("data/intermediate/samples")
    )
    
    if common_peaks.empty:
        print(f"   âœ— Pas de pics trouvÃ©s pour {base_name}")
        return
    
    # 2. Soustraction du blank
    if not blank_peaks.empty:
        print("\nğŸ§¹ Soustraction des pics du blank...")
        clean_peaks = subtract_blank_peaks(common_peaks, blank_peaks)
        pics_retires = len(common_peaks) - len(clean_peaks)
        pourcentage = (pics_retires / len(common_peaks) * 100) if len(common_peaks) > 0 else 0
        print(f"   âœ“ {pics_retires} pics retirÃ©s ({pourcentage:.1f}%)")
        print(f"   âœ“ {len(clean_peaks)} pics aprÃ¨s soustraction du blank")
    else:
        clean_peaks = common_peaks
        
    if clean_peaks.empty:
        print(f"   âœ— Pas de pics aprÃ¨s soustraction du blank pour {base_name}")
        output_file = output_dir / "common_peaks.parquet"
        pd.DataFrame().to_parquet(output_file)
        return
        
    # 3. Calibration CCS sur les pics nettoyÃ©s
    print("\nğŸ”µ Calibration CCS...")
    peaks_with_ccs = calibrator.calculate_ccs(clean_peaks)
    if not peaks_with_ccs.empty:
        print(f"   âœ“ CCS calculÃ©es pour {len(peaks_with_ccs)} pics")
        print(f"   âœ“ Plage de CCS: {peaks_with_ccs['CCS'].min():.2f} - {peaks_with_ccs['CCS'].max():.2f} Ã…Â²")
        print(f"   âœ“ CCS moyenne: {peaks_with_ccs['CCS'].mean():.2f} Ã…Â²")
        
        # Sauvegarde finale
        output_file = output_dir / "common_peaks.parquet"
        peaks_with_ccs.to_parquet(output_file)
        print(f"   âœ“ Pics finaux sauvegardÃ©s dans {output_file}")

def generate_visualizations(output_dir: Path) -> None:
    """GÃ©nÃ¨re toutes les visualisations de la pipeline."""
    try:
        print("\nğŸ“Š GÃ©nÃ©ration des visualisations...")
        output_dir.mkdir(exist_ok=True)
        
        # Les fichiers d'identifications sont dans output_dir/feature_matrix/
        identifications_file = output_dir / "feature_matrix" / "feature_identifications.parquet"
        if not identifications_file.exists():
            raise FileNotFoundError(f"Fichier d'identifications non trouvÃ©: {identifications_file}")

        # Plot du nombre total de molÃ©cules par Ã©chantillon
        fig = plot_unique_molecules_per_sample(output_dir)
        fig.savefig(output_dir / "molecules_per_sample.png")
        plt.close()

        # Plot des molÃ©cules niveau 1
        fig = plot_level1_molecules_per_sample(output_dir)
        fig.savefig(output_dir / "level1_molecules_per_sample.png")
        plt.close()

        # Bubble plot niveau 1
        fig_bubble = plot_level1_molecule_distribution_bubble(output_dir)
        fig_bubble.savefig(output_dir / "level1_molecule_distribution_bubble.png",
                         bbox_inches='tight',
                         dpi=300)
        plt.close()

        # TIC - utilise les donnÃ©es d'entrÃ©e
        plot_tics_interactive(Path("data/input/samples"), output_dir)

        # Heatmaps
        fig_similarity = plot_sample_similarity_heatmap(output_dir)
        fig_similarity.savefig(output_dir / "sample_similarity_heatmap_all.png")
        plt.close()

        # Heatmaps par niveau de confiance
        fig_similarity_l1 = plot_sample_similarity_heatmap_by_confidence(
            output_dir, 
            confidence_levels=[1],
            title_suffix=" - Niveau 1"
        )
        fig_similarity_l1.savefig(output_dir / "sample_similarity_heatmap_level1.png")
        plt.close()

        fig_similarity_l12 = plot_sample_similarity_heatmap_by_confidence(
            output_dir, 
            confidence_levels=[1, 2],
            title_suffix=" - Niveaux 1 et 2"
        )
        fig_similarity_l12.savefig(output_dir / "sample_similarity_heatmap_level1_2.png")
        plt.close()

        fig_similarity_l123 = plot_sample_similarity_heatmap_by_confidence(
            output_dir, 
            confidence_levels=[1, 2, 3],
            title_suffix=" - Niveaux 1, 2 et 3"
        )
        fig_similarity_l123.savefig(output_dir / "sample_similarity_heatmap_level1_2_3.png")
        plt.close()

        print(f"   âœ“ Visualisations sauvegardÃ©es dans {output_dir}")

    except Exception as e:
        print(f"Erreur lors de la crÃ©ation des visualisations: {str(e)}")
        raise


def main() -> None:
    """Point d'entrÃ©e principal de la pipeline."""
    setup_logging()
    print("\nğŸš€ DÃ‰MARRAGE DE LA PIPELINE D'ANALYSE")
    print("=" * 80)

    try:
        # 1. Chargement des donnÃ©es de calibration CCS
        print("\nğŸ“ˆ Chargement des donnÃ©es de calibration CCS...")
        calibration_file = Path("data/input/calibration/CCS_calibration_data.csv")
        if not calibration_file.exists():
            raise FileNotFoundError(f"Fichier de calibration non trouvÃ© : {calibration_file}")
        calibrator = CCSCalibrator(calibration_file)
        print("   âœ“ DonnÃ©es de calibration chargÃ©es avec succÃ¨s")

        # 2. Initialisation de l'identificateur
        print("\nğŸ“š Initialisation de l'identification...")
        identifier = CompoundIdentifier()
        print("   âœ“ Base de donnÃ©es chargÃ©e avec succÃ¨s")

        # 3. Traitement des blanks
        print("\nğŸ“ Recherche des blanks...")
        blank_dir = Path("data/input/blanks")
        blank_files = list(blank_dir.glob("*.parquet"))
        blank_peaks = process_blank_files(blank_files)

        # 4. Traitement des Ã©chantillons
        print("\nğŸ“ Recherche des fichiers d'Ã©chantillons...")
        samples_dir = Path(Config.INPUT_SAMPLES)
        sample_files = list(samples_dir.glob("*.parquet"))

        if not sample_files:
            raise ValueError("Aucun fichier d'Ã©chantillon trouvÃ©.")
            
        replicate_groups = group_replicates(sample_files)
        
        print(f"   âœ“ {len(replicate_groups)} Ã©chantillons trouvÃ©s:")
        for base_name, replicates in replicate_groups.items():
            print(f"      - {base_name}: {len(replicates)} rÃ©plicat(s)")

        # Traitement de chaque Ã©chantillon
        for base_name, replicates in replicate_groups.items():
            process_full_sample(
                base_name,
                replicates,
                blank_peaks,
                calibrator,
                Path("data/intermediate/samples")
            )

        # 5. Feature Matrix et identification
        print("\nğŸ“Š CrÃ©ation de la matrice des features...")
        create_feature_matrix(
            input_dir=Path("data/intermediate/samples"),
            output_dir=Path("output/feature_matrix"),
            identifier=identifier
        )

        # 6. Visualisations
        print("\nğŸ“Š GÃ©nÃ©ration des visualisations...")
        output_dir = Path("output")
        generate_visualizations(output_dir)
        
        print("\nğŸ“Š Analyse des similaritÃ©s entre Ã©chantillons...")
        analyze_and_save_clusters(output_dir)
        
        print(f"   âœ“ Visualisations sauvegardÃ©es dans {output_dir}")

        # Fin du traitement
        print("\nâœ… TRAITEMENT TERMINÃ‰ AVEC SUCCÃˆS")
        print("=" * 80)

    except Exception as e:
        print("\nâŒ ERREUR DANS LA PIPELINE")
        logger.error(f"Erreur pipeline : {str(e)}")
        raise

if __name__ == "__main__":
    main()