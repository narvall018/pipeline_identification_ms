#main.py
#-*- coding:utf-8 -*-


# Importation des modules
import logging
import warnings
import pandas as pd
from pathlib import Path
from typing import Union
from scripts.config.config import Config
from scripts.utils.io_handlers import read_parquet_data, save_peaks
from scripts.processing.peak_detection import prepare_data, detect_peaks, cluster_peaks
from scripts.processing.ccs_calibration import CCSCalibrator
from scripts.processing.identification import CompoundIdentifier
from scripts.processing.ms2_extraction import extract_ms2_for_matches
from scripts.processing.ms2_comparaison import add_ms2_scores
from scripts.visualization.plotting import plot_unique_molecules_per_sample


# Suppression des warnings pandas
warnings.filterwarnings('ignore')
pd.options.mode.chained_assignment = None


# Initialiser le logger
logger = logging.getLogger(__name__)


def setup_logging() -> None:
    """
    Configure le systÃ¨me de logging pour enregistrer les Ã©vÃ©nements dans un fichier.

    Args:
        None

    Returns:
        None
    """
    # DÃ©finition du rÃ©pertoire oÃ¹ les logs seront enregistrÃ©s
    log_dir = Path("logs")

    # CrÃ©e le rÃ©pertoire si nÃ©cessaire
    log_dir.mkdir(exist_ok=True)

    # Configure le systÃ¨me de logging
    logging.basicConfig(
        filename=log_dir / "peak_detection.log",  # Fichier oÃ¹ les logs seront Ã©crits
        level=logging.INFO,                       # Niveau de log minimum (INFO et supÃ©rieur)
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # Format des messages de log
        force=True                                # Remplace toute configuration de logging prÃ©cÃ©dente
    )

    # Ajoute un message indiquant que le logging a Ã©tÃ© configurÃ©
    logging.info("Logging configurÃ© avec succÃ¨s.")


def process_file(
			file_path: Union[str, Path],
			calibrator: CCSCalibrator,
			identifier: CompoundIdentifier,
			data_type: str = 'samples',
			total_files: int = 1,
			current_file: int = 1
	) -> None:
	"""
	Traite un fichier d'Ã©chantillon pour dÃ©tecter les pics, calibrer les CCS, et identifier les composÃ©s.

	Args:
		file_path (Union[str, Path]): Chemin vers le fichier de l'Ã©chantillon Ã  traiter.
		calibrator (CCSCalibrator): Instance du calibrateur CCS.
		identifier (CompoundIdentifier): Instance pour l'identification des composÃ©s.
		data_type (str): Type de donnÃ©es (par dÃ©faut 'samples').
		total_files (int): Nombre total de fichiers Ã  traiter.
		current_file (int): Index du fichier en cours.

	Returns:
		None
	"""
	try:
		sample_name = Path(file_path).stem

		print(f"\n{'=' * 80}")
		print(f"TRAITEMENT DE {sample_name} ({current_file}/{total_files})")
		print(f"{'=' * 80}")

		# Lecture des donnÃ©es
		print("\nğŸ“Š Lecture des donnÃ©es...")
		data, metadata = read_parquet_data(file_path)
		print(f"   âœ“ DonnÃ©es chargÃ©es : {len(data)} lignes")

		# PrÃ©paration des donnÃ©es MS1
		print("\nğŸ” PrÃ©paration des donnÃ©es MS1...")
		processed_data = prepare_data(data)
		if processed_data is None or processed_data.empty:
			print("   âœ— Aucune donnÃ©e MS1 valide")
			return

		print(f"   âœ“ DonnÃ©es prÃ©parÃ©es : {len(processed_data)} lignes")

		# DÃ©tection des pics
		print("\nğŸ¯ DÃ©tection des pics...")
		peaks = detect_peaks(processed_data)
		if peaks.empty:
			print("   âœ— Aucun pic dÃ©tectÃ©")
			return

		print(f"   âœ“ Pics dÃ©tectÃ©s : {len(peaks)}")
		save_peaks(peaks, sample_name, "peaks", data_type, metadata)

		# Clustering des pics
		print("\nğŸ”„ Clustering des pics...")
		clustered_peaks = cluster_peaks(peaks)
		if clustered_peaks.empty:
			print("   âœ— Pas de pics aprÃ¨s clustering")
			return

		print(f"   âœ“ Pics aprÃ¨s clustering : {len(clustered_peaks)}")
		save_peaks(clustered_peaks, sample_name, "clustered_peaks", data_type, metadata)

		# Calibration CCS
		print("\nğŸ”µ Calibration CCS...")
		peaks_with_ccs = calibrator.calculate_ccs(clustered_peaks)
		if peaks_with_ccs.empty:
			print("   âœ— Erreur dans le calcul des CCS")
			return

		print(f"   âœ“ CCS calculÃ©es pour {len(peaks_with_ccs)} pics")
		save_peaks(peaks_with_ccs, sample_name, "ccs_peaks", data_type, metadata)

		# Identification des composÃ©s
		print("\nğŸ” Identification des composÃ©s...")
		identification_dir = Path(f"data/intermediate/{data_type}/{sample_name}/ms1/identifications")
		matches_df = identifier.identify_compounds(peaks_with_ccs, identification_dir)

		if matches_df is None or matches_df.empty:
			print("   âœ— Aucun match trouvÃ©")
			return

		print(f"   âœ“ Matches trouvÃ©s : {len(matches_df)}")
		matches_df = extract_ms2_for_matches(matches_df, file_path, identification_dir)

		# RÃ©sumÃ©
		print(f"\nâœ¨ Traitement terminÃ© pour {sample_name}")
		print(f"   - Pics dÃ©tectÃ©s : {len(peaks)}")
		print(f"   - Pics aprÃ¨s clustering : {len(clustered_peaks)}")
		print(f"   - Pics avec CCS : {len(peaks_with_ccs)}")
		if matches_df is not None and not matches_df.empty:
			print(f"   - Matches avec MS2 : {len(matches_df)}")
		print(f"{'=' * 80}")

	except Exception as e:
		print(f"\nâŒ Erreur lors du traitement de {sample_name}")
		logger.error(f"Erreur traitement {file_path} : {str(e)}")
		raise


def main() -> None:
	"""
	Point d'entrÃ©e principal pour la pipeline d'analyse. Configure le logger,
	initialise les composants, traite les fichiers d'Ã©chantillons, et gÃ©nÃ¨re les visualisations.
	"""
	setup_logging()
	print("\nğŸš€ DÃ‰MARRAGE DE LA PIPELINE D'ANALYSE")
	print("=" * 80)

	try:
		# Initialisation du calibrateur CCS
		print("\nğŸ“ˆ Chargement des donnÃ©es de calibration CCS...")
		calibration_file = Path("data/input/calibration/CCS_calibration_data.csv")
		if not calibration_file.exists():
			raise FileNotFoundError(f"Fichier de calibration non trouvÃ© : {calibration_file}")

		calibrator = CCSCalibrator(calibration_file)
		print("   âœ“ DonnÃ©es de calibration chargÃ©es avec succÃ¨s")

		# Initialisation de l'identificateur
		print("\nğŸ“š Initialisation de l'identification...")
		identifier = CompoundIdentifier()
		print("   âœ“ Base de donnÃ©es chargÃ©e avec succÃ¨s")

		# Traitement des fichiers d'Ã©chantillons
		samples_dir = Path(Config.INPUT_SAMPLES)
		sample_files = list(samples_dir.glob("*.parquet"))
		if not sample_files:
			raise ValueError("Aucun fichier d'Ã©chantillon trouvÃ©.")

		print(f"\nğŸ“ Traitement de {len(sample_files)} Ã©chantillon(s)")
		for idx, file_path in enumerate(sample_files, 1):
			process_file(file_path, calibrator, identifier, 'samples', len(sample_files), idx)

		# Calcul des scores MS2
		print("\nğŸ“Š Calcul des scores de similaritÃ© MS2...")
		for sample_path in Path("data/intermediate/samples").glob("*/ms1/identifications/all_matches.parquet"):
			add_ms2_scores(sample_path, identifier)

		# GÃ©nÃ©ration de visualisations
		print("\nğŸ“Š GÃ©nÃ©ration des visualisations...")
		output_dir = Path("output")
		output_dir.mkdir(exist_ok=True)

		fig = plot_unique_molecules_per_sample("data/intermediate/samples")
		fig.savefig(output_dir / "molecules_per_sample.png")
		print("   âœ“ Visualisation sauvegardÃ©e dans output/molecules_per_sample.png")

		print("\nâœ… TRAITEMENT TERMINÃ‰ AVEC SUCCÃˆS")
		print("=" * 80)

	except Exception as e:
		print("\nâŒ ERREUR DANS LA PIPELINE")
		logger.error(f"Erreur pipeline : {str(e)}")
		raise

if __name__ == "__main__":
	main()
